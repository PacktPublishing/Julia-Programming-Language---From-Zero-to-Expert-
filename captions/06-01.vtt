WEBVTT

00:00.750 --> 00:06.930
In this section of the course, we are going to use Julia to build neural networks from scratch using

00:06.930 --> 00:09.480
the machine learning laboratory in Julia, which is called Flux.

00:09.870 --> 00:12.600
Now, this is not a course on machine learning.

00:12.600 --> 00:17.100
So I'm assuming that you're already a little bit familiar with neural networks and you've worked with

00:17.310 --> 00:21.690
other laboratories which work with neural networks if you haven't worked with neural networks before.

00:21.720 --> 00:23.400
There are tons of tutorials on the Internet.

00:23.410 --> 00:27.360
You can look them up and get familiar with them before you start with Julia, because Julia is kind

00:27.360 --> 00:28.740
of like advanced machine learning.

00:30.000 --> 00:33.210
It starts from zero, but it comes up to state of the art.

00:33.210 --> 00:38.490
Really quickly, I'm going to give you a crash course on the basic concepts of machine learning, but

00:38.490 --> 00:40.900
only the concepts that are relevant to our discussion over here.

00:41.490 --> 00:44.880
So to get started, all neural networks are two buses.

00:45.120 --> 00:49.230
One is the forward pass in which you take some attributes of the data that you have.

00:49.410 --> 00:54.200
You multiply them with some weights and you apply some squishing function and you get an output back.

00:54.540 --> 01:01.200
OK, so for instance, you might have W into X plus B is equal to this Z value overhead and you can

01:01.200 --> 01:03.690
apply a sigmoid or an activation function to it.

01:03.840 --> 01:06.120
That is your forward, but it's fairly straightforward.

01:06.540 --> 01:11.700
Then you do a backward pass in which you calculate the derivative of this loss that you've calculated

01:12.780 --> 01:17.730
or the error that you have with respect to all the parameters that you have for the model.

01:17.760 --> 01:23.940
So the W that you had over here, that needs to change so that when it is multiplied to X, the loss

01:23.940 --> 01:24.720
is minimized.

01:25.590 --> 01:34.620
And you do that using the derivative of the loss with respect to W and then you updated using the gradient

01:34.620 --> 01:38.760
descent update function, which is essentially the old value of W or Thida.

01:39.900 --> 01:46.980
You take that old value, you subtract the derivative with respect to that multiplied by some learning

01:46.980 --> 01:48.390
parameter, which is called Alpha.

01:48.420 --> 01:51.240
OK, so these are the three things that you have to do.

01:51.250 --> 01:52.380
So let's put them together.

01:52.980 --> 01:56.390
So you have the forward pass, which is essentially just a mathematical function.

01:56.400 --> 01:57.390
It's not complicated.

01:57.660 --> 02:01.630
You have the backward pass and then you have the parameter addition step.

02:01.650 --> 02:03.290
So this is fairly straightforward.

02:03.300 --> 02:04.530
This is fairly straightforward.

02:04.800 --> 02:10.170
The backward pass is complicated because in this you have to have a very complicated mathematical function

02:10.290 --> 02:12.410
and you have to calculate the derivatives for that.

02:12.570 --> 02:14.920
And this is where people struggle with machine learning.

02:15.480 --> 02:19.050
You have to calculate derivatives with respect to millions of different parameters.

02:19.050 --> 02:21.650
And these are partial derivatives, extremely complicated.

02:22.050 --> 02:24.600
So easy, easy and difficult.

02:25.650 --> 02:33.090
The way Julia works is it allows you to specify the forward pass and the abduction step, but it takes

02:33.090 --> 02:36.960
care of the backward pass, which is the difficult part automatically for you.

02:37.200 --> 02:40.860
And that makes our life very easy as machine learning developers.

02:41.160 --> 02:44.400
So keeping this in mind, let's go ahead and see how Julia does this.

02:44.580 --> 02:49.440
So we'll start off with a very basic mathematical function, calculate the derivatives and then move

02:49.440 --> 02:51.030
onto the ablation step as well.

02:51.870 --> 02:54.630
So the library is flux.

02:54.630 --> 02:59.660
If you haven't installed it, you can install it using add flux and then start with using flux.

03:00.030 --> 03:02.660
So this is our dommy function.

03:02.670 --> 03:09.930
So 3x squared plus two X plus one, just a basic function which will give you a derivative or gradient

03:09.930 --> 03:11.160
of six X plus two.

03:11.160 --> 03:12.750
But we're not going to calculate this.

03:12.750 --> 03:14.990
We're going to get Juliar to calculate it for us.

03:15.330 --> 03:20.910
So this is our function and all you have to do is call the built in function in flux, which is called

03:20.910 --> 03:26.970
gradient, and tell it to calculate the gradient of F with respect to its one parameter, but for the

03:26.970 --> 03:28.200
value of X2.

03:28.320 --> 03:36.210
So this is the derivative of F with respect to X when X is equal to do that is what this is the gradient

03:36.210 --> 03:39.630
of F with respect to X when you want to do so.

03:39.630 --> 03:42.310
Six plus two, that is going to be evaluated.

03:42.780 --> 03:46.840
So six into two is twelve plus two is four.

03:46.860 --> 03:48.540
So we get the right answer over here.

03:49.030 --> 03:53.580
So that's all the gradient part, which was the difficult thing we did not have to do by hand.

03:53.940 --> 03:54.930
So I brought this here.

03:54.930 --> 03:56.010
But we don't need this.

03:56.040 --> 03:56.880
This is just a comment.

03:57.030 --> 04:03.180
OK, we can go ahead and make this a little cleaner because this is in a tuple and we need a value so

04:03.180 --> 04:04.530
we can define a new function.

04:04.530 --> 04:05.610
D.F. of X.

04:05.610 --> 04:09.390
So this DF is just for our ease of understanding.

04:09.690 --> 04:10.610
It can be any name.

04:11.010 --> 04:13.800
So the F effects is equal to this gradient function.

04:14.100 --> 04:15.380
We're going to multiply this.

04:15.510 --> 04:16.800
So this is the value of the function.

04:16.800 --> 04:17.170
Right.

04:17.190 --> 04:21.800
If you don't remember, recall that this is a function which is defined in a single line.

04:22.320 --> 04:25.500
So this is the function name and the parameter, and this is a function body.

04:26.010 --> 04:32.210
What the function body does is it caused the gradient of F with respect to X. So this value and returns

04:32.220 --> 04:38.280
the first elements of 14 back so you can have this and then you called at two, you get the value for

04:38.460 --> 04:38.720
back.

04:38.930 --> 04:45.750
OK, so now what D.F. is is the derivative of F with respect to X at this particular value.

04:46.140 --> 04:47.730
OK, so this is very easy to work with.

04:48.630 --> 04:49.830
Make sure that you understand this.

04:49.830 --> 04:56.100
If you don't go back around this again, calculate this for different values of X so that you are crystal

04:56.100 --> 04:56.730
clear on this.

04:57.150 --> 04:59.400
So we're going to build on top of this now.

04:59.400 --> 05:00.060
Let's go ahead and.

05:00.160 --> 05:04.540
Take a look at functions which have multiple parameters, so, for instance, we have a function F which

05:04.540 --> 05:08.170
takes two vectors as it's input X and Y.

05:08.500 --> 05:15.160
OK, so what the function itself does is it calculates the difference of corresponding elements in X

05:15.160 --> 05:17.230
and Y and then squares them.

05:17.410 --> 05:18.950
OK, so your typical means coordinator.

05:19.000 --> 05:21.660
OK, this is a simple function definition.

05:21.670 --> 05:23.130
Nothing complicated going on over here.

05:23.350 --> 05:26.520
The only thing is X and Y are both going to be vectors.

05:26.830 --> 05:27.940
OK, no problem.

05:28.390 --> 05:32.680
Now we are going to take this W as X and B as Y.

05:34.060 --> 05:36.220
There are two vectors, this and this.

05:36.610 --> 05:42.910
We are going to calculate the derivative of F with respect to these two vectors.

05:43.180 --> 05:43.460
OK.

05:43.720 --> 05:49.540
And because these are vectors, the derivative is going to be piecewise or partial derivative with respect

05:49.540 --> 05:50.740
to this guy separately.

05:50.740 --> 05:51.490
And this guy.

05:51.490 --> 05:52.090
And this guy.

05:52.090 --> 05:52.620
And this guy.

05:52.630 --> 05:55.660
So there are going to be four partial derivatives over here.

05:55.940 --> 05:57.550
And if you don't recall calculus.

05:57.580 --> 05:58.440
Don't worry about it.

05:58.450 --> 06:02.440
Just remember that we are calculating this derivative stuff.

06:02.470 --> 06:02.770
OK.

06:03.670 --> 06:08.890
So if you have one W here and another W here, we are going to calculate the derivative of this loss

06:08.890 --> 06:11.670
with respect to this W separately and this W separately.

06:12.160 --> 06:17.830
So if this was a vector W1 W2, we are going to calculate derivative with respect to W one separately

06:17.830 --> 06:22.210
and W do separately and then for instance here Q1, Q2 separately.

06:22.210 --> 06:24.610
So everything is going to be calculated separately.

06:25.330 --> 06:29.500
Right, because we need to use those to apply this parameter instead.

06:29.740 --> 06:31.740
OK, so let's go ahead and do this.

06:31.750 --> 06:34.160
So the function is this guy over here.

06:34.180 --> 06:39.700
So this is the forward pass and backward passes the calculation of gradient with respect to each different

06:39.700 --> 06:40.060
value.

06:40.360 --> 06:42.820
So we are going to do that and we get four different values.

06:42.820 --> 06:50.140
So what this is saying is the gradient or derivative of F with respect to these two values and because

06:50.140 --> 06:56.380
these are vectors, we are going to calculate the derivative of F with respect to w w w one is equal

06:56.380 --> 06:56.950
to do so.

06:56.950 --> 06:58.270
That is the zero one.

06:58.270 --> 07:03.070
W two is equal to one and that is this B1 and B2.

07:03.460 --> 07:05.800
So all of these are calculated automatically for us.

07:06.710 --> 07:14.810
Now, this is slightly difficult to work with, so Flux provides an alternative syntax, so here you

07:14.810 --> 07:16.430
have to calculate what one is.

07:16.430 --> 07:21.440
What do is what we want is what we do is we have an easier method for doing this.

07:21.590 --> 07:23.930
What we can say is we can define a new function.

07:24.860 --> 07:26.510
This is going to be the body of the function.

07:26.690 --> 07:32.390
What this is going to do is it's going to calculate the gradient with respect to these parameters,

07:32.390 --> 07:34.160
WNBA for this function.

07:35.320 --> 07:37.420
So the syntax looks slightly convoluted.

07:37.840 --> 07:44.920
It's the exact same thing, the gradient with respect to the parameters WFB of this function is going

07:44.920 --> 07:47.340
to be calculated and it's going to be saved in G.S..

07:47.920 --> 07:50.070
So when you do that, you get some great games back.

07:50.230 --> 07:53.060
If you say Gowda grades, it gives you a huge mess.

07:53.080 --> 07:59.500
So it's kind of complicated, but the point of using this syntax is now you can say GSW and you get

07:59.500 --> 08:00.180
012.

08:00.430 --> 08:03.700
So the same value and back out you can simply use.

08:05.960 --> 08:09.330
This to get the one and this to get W2.

08:09.650 --> 08:15.740
OK, so it works out similarly, you can go ahead and get these derivatives with respect to B, which

08:15.740 --> 08:16.620
are these guys over here?

08:16.910 --> 08:18.050
So same idea.

08:18.140 --> 08:19.340
You have a function over here.

08:20.120 --> 08:25.190
You need the derivatives, partial derivatives of this with respect to all the parameters you use this

08:25.190 --> 08:25.520
index.

08:25.550 --> 08:30.140
OK, when we go ahead with we are going to get rid of even this stuff over here.

08:30.140 --> 08:32.570
And the final syntax is going to be very easy.

08:32.750 --> 08:38.090
But we are building this from scratch so that if you have to create your own models later on, you know

08:38.090 --> 08:38.870
how to do that.

08:38.950 --> 08:42.170
OK, the final syntax of flux is very clean, very easy to work with.

08:42.440 --> 08:46.550
This is just the building blocks and that's why it looks slightly messy in the beginning.

08:46.580 --> 08:49.370
OK, now let's go out and create a basic model for for.

08:49.940 --> 08:51.740
So we are going to create this stuff.

08:51.990 --> 08:55.960
We are going to have some X is going in and some bits are going to be able to play with that.

08:56.240 --> 08:57.300
So we have some WS.

08:57.310 --> 08:58.970
So these are the random bits.

08:59.000 --> 09:06.590
So these so this is a matrix two by five and the Bias's are a vector.

09:06.740 --> 09:08.510
OK, so what is the prediction?

09:08.600 --> 09:13.790
You would have seen this before we multiply with X and then add B to it.

09:15.020 --> 09:21.890
OK, so the prediction of X's W, X plus B, this is a basic forward pass for a single neuron.

09:22.380 --> 09:23.810
But what is the loss?

09:24.620 --> 09:32.030
The loss function is y is you do the prediction and then you calculate the squared error and you sum

09:32.030 --> 09:33.620
over all the different data points that you have.

09:33.710 --> 09:36.050
That is your loss, just a basic loss.

09:37.130 --> 09:38.120
By the way, we write this.

09:38.120 --> 09:44.480
If I had as you write Y and then you say a slash had hit tab and it's going to convert into Lahat,

09:45.080 --> 09:50.190
OK, this looks very similar to the mathematical notation which appears in the paper.

09:50.210 --> 09:51.500
So Julia prefers this.

09:51.620 --> 09:53.060
So you should get used to doing this.

09:53.660 --> 09:57.010
Anyway, we have the last function and we have debates.

09:57.230 --> 10:02.690
Now what we want to do is we want to calculate the loss for particular values for X and Y is going to

10:02.690 --> 10:03.940
be around five and two.

10:03.950 --> 10:07.400
So this is some dummy data are X values are going to be.

10:08.430 --> 10:11.070
These and our values are going to be the story here.

10:11.160 --> 10:15.340
OK, so X and Y, we can calculate the loss of X and Y.

10:16.110 --> 10:19.510
So what loss does is we go over there.

10:19.560 --> 10:23.180
We predict a vie for these X values.

10:23.190 --> 10:30.510
So for these guys over here, we predict away and calculate what the difference is because W was initialized

10:30.990 --> 10:33.630
as a random, we are going to get some random loss of that.

10:33.630 --> 10:34.480
He had three point six.

10:34.530 --> 10:35.160
It's not good.

10:35.160 --> 10:39.810
Obviously we are going to improve this by applying the backward pass for that.

10:39.810 --> 10:40.560
We need the gradient.

10:40.560 --> 10:43.140
So we use this index gradient parameters.

10:44.060 --> 10:50.750
For this lost function, so the gradient are derivative of this lost function with respect to W and

10:50.750 --> 10:51.980
B is going to be needed.

10:52.340 --> 10:53.960
OK, we do that.

10:54.140 --> 10:57.530
And because this is very common, we have an alternative syntax as well.

10:57.800 --> 10:59.750
So you can use this exact same thing.

10:59.780 --> 11:07.210
It does the exact same thing is that you have a function which is anonymous function for loss and this

11:07.220 --> 11:07.680
guy over here.

11:07.700 --> 11:09.170
So this is an alternative syntax.

11:09.810 --> 11:11.840
It comes up in Julia's documentation.

11:11.840 --> 11:13.480
So it's included over here.

11:13.640 --> 11:16.910
But if you if you understand this, this is exactly the same thing.

11:16.940 --> 11:20.780
OK, now you can say dot grads and you get this big old mess.

11:21.140 --> 11:25.910
But the point is we can say the derivatives for W are JSW.

11:26.270 --> 11:28.370
OK, so this is true.

11:28.680 --> 11:31.720
But now we have the JSW.

11:32.210 --> 11:33.440
This is this guy over here.

11:33.840 --> 11:42.350
OK, so the derivative of the lost function with respect to this particular vate that is in W hat over

11:42.350 --> 11:43.350
here or the bluebottle.

11:43.630 --> 11:45.600
OK, we are going to apply this.

11:45.620 --> 11:46.580
So this is your alpha.

11:47.490 --> 11:48.060
W.

11:49.090 --> 11:55.390
Not minus is equal to this guy over here, so all value minus alpha into the derivative.

11:55.960 --> 11:58.300
We do that so all the weights are updated.

11:59.810 --> 12:04.670
If you calculate the loss now, you will see that the loss has gone down because we have applied the

12:04.670 --> 12:06.080
backward pass once.

12:07.160 --> 12:12.740
We can also calculate the derivatives with respect to be so that is this and we update the biases as

12:12.740 --> 12:12.930
well.

12:12.950 --> 12:14.990
And now the loss is going to go further down.

12:16.160 --> 12:21.640
We can go ahead and do another pass by calculating the derivatives with respect to WFB again.

12:22.070 --> 12:23.450
So we're going to do this.

12:23.840 --> 12:30.410
And if you do this now, it's going to update and you update this and you calculate the loss, it goes

12:30.410 --> 12:30.890
further down.

12:30.890 --> 12:35.570
You calculate the derivatives with respect to B, update your B and you calculate the loss.

12:35.570 --> 12:36.470
It goes further down.

12:37.010 --> 12:43.220
So our boss is going to be calculate the gradient with the new WFB values and update those.

12:43.820 --> 12:46.430
And that's your basic neural network attrition.

12:47.150 --> 12:51.170
Calculate the derivative with respect to current values of rates and update the rates.

12:51.450 --> 12:52.140
That is the thing.

12:52.610 --> 12:54.790
Let's go ahead and revise this whole thing again.

12:54.830 --> 12:55.730
It's very dense.

12:55.880 --> 12:58.610
So you might have to look at this again and again.

12:58.880 --> 13:02.570
But this forms the building block of the Flux Library.

13:02.570 --> 13:06.530
And as you can see, if you get rid of everything, it's just three or four lines of code.

13:06.950 --> 13:08.630
So it's going to take a look at this.

13:09.530 --> 13:16.370
So these are random words and biases, we initialize them randomly afterwards, forward pass is simply

13:16.370 --> 13:23.720
the multiplication of W and then addition of loss is just the mean squared error, which we normally

13:23.720 --> 13:24.020
have.

13:25.850 --> 13:32.970
X and Y are going to be some dummy data, so some dumphy values for X and Dumitru, values for Y.

13:33.190 --> 13:34.520
OK, these are the values.

13:35.000 --> 13:36.440
First we look at what the law says.

13:36.710 --> 13:43.070
Then we calculate the derivative with respect to the laws and then we update the weights first and then

13:43.070 --> 13:43.700
devices.

13:43.800 --> 13:44.690
That's all there is to it.

13:44.720 --> 13:49.880
So going back to the figure, we do the forward pass, we calculate the derivative and we update the

13:50.300 --> 13:52.760
weights based on the derivative that we've calculated.

13:53.130 --> 13:55.400
We do this again and again and it becomes very clear.

13:55.640 --> 13:58.900
So the next word and put this in one cell so that you understand what's happened.

13:59.330 --> 14:00.920
So we have the weights.

14:01.190 --> 14:02.420
So you go.

14:03.890 --> 14:10.820
I'm going to do this in real time so that you can see so these are our rates and biases, I predict

14:10.820 --> 14:11.720
function is.

14:14.400 --> 14:14.850
This.

14:17.490 --> 14:18.000
Then.

14:20.440 --> 14:22.180
We have our lost function.

14:24.070 --> 14:25.630
This would be our last function.

14:28.590 --> 14:30.250
Then we need the ingredients.

14:32.950 --> 14:34.330
You can use either of these two.

14:35.240 --> 14:38.290
OK, so let's go ahead and put these in a separate cell.

14:39.670 --> 14:44.470
We take our data as well, so our data is going to be this.

14:46.510 --> 14:50.740
Let's put this over here, so I'm putting everything that has to be repeated here and everything that

14:50.740 --> 14:52.550
is done once over here, OK?

14:53.350 --> 14:54.580
Finally, we have the.

14:58.910 --> 15:08.060
It's over here and we are going to update the weights using this similarly for Bias's, we have the

15:08.060 --> 15:10.340
calculation of biases and oblation as well.

15:10.940 --> 15:15.200
OK, and then we output the loss with respect to X and Y.

15:15.610 --> 15:17.780
OK, so hopefully this makes sense.

15:18.090 --> 15:21.360
Let's go ahead and output the loss initially as well.

15:21.890 --> 15:22.770
So there you go.

15:23.120 --> 15:24.170
This is our initial loss.

15:24.440 --> 15:26.090
Now let's go ahead and do the aggression.

15:26.420 --> 15:27.260
Loss goes down.

15:27.410 --> 15:32.420
You do it again and loss goes further down and you keep doing that and loss is going to keep going down.

15:32.630 --> 15:32.950
OK.

15:38.940 --> 15:45.180
I hope you understand this forward pass, backward pass and the abduction, so that's all there is to

15:45.180 --> 15:45.330
it.

15:45.720 --> 15:48.150
This is all you need to write a simple neural network.

15:48.150 --> 15:53.730
And Julia, if you've done this stuff with any other library, you would notice that this is extremely

15:53.730 --> 15:54.090
easy.

15:54.120 --> 15:56.870
Much easier to work with than any other library.

15:57.180 --> 16:03.180
So in the next video, we are going to take these basic concepts and build a multi-level neural network

16:03.420 --> 16:04.530
instead of a single neuron.
