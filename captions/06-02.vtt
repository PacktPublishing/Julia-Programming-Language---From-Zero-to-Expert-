WEBVTT

00:00.680 --> 00:02.570
We built a single neuron in the previous video.

00:02.690 --> 00:07.790
In this video, we are going to join different neurons together to create a multilayer Perceptron.

00:08.120 --> 00:13.190
So as before, we are going to start with using flux and we are going to have a very simple model.

00:13.190 --> 00:15.920
So we are going to have one by one for the first layer.

00:16.340 --> 00:19.130
These are the words for the first layer and these are devices.

00:19.370 --> 00:26.790
And our model is very simply or our layer is very simply going to be layer one is WAAX plus B1.

00:26.810 --> 00:28.010
So that's our first layer.

00:28.130 --> 00:36.090
OK, now typically after a layer has done the linear computation, you apply some sort of a squishing

00:36.110 --> 00:37.240
function to that.

00:37.520 --> 00:40.420
And one of the most typical functions is the sigmoid function.

00:40.430 --> 00:41.930
So that is built into flux.

00:42.230 --> 00:48.260
So you can say sigmoid sigma of zero is zero point five sigma if one is point seven and Sigma minus

00:48.260 --> 00:49.370
one is point two.

00:49.610 --> 00:54.020
So you can type this as Sigma Hattab and it turns into sigma.

00:55.070 --> 00:56.440
So this is building.

00:56.870 --> 01:02.990
Now what we can say is you define L1 X as first, you do the layer one computation.

01:03.020 --> 01:07.880
So this computation over here and then whatever output you get out, you apply squishing function to

01:07.880 --> 01:08.010
it.

01:08.300 --> 01:08.630
OK.

01:09.660 --> 01:12.930
And this is the broadcast, so we applied this question to all the different values.

01:14.040 --> 01:15.150
It works out very well.

01:15.300 --> 01:17.670
OK, so that is all there is to it, very clean.

01:17.700 --> 01:18.280
Nice code.

01:18.660 --> 01:20.530
OK, so we have 11 now.

01:20.550 --> 01:23.540
Similarly, we can define and do as W2 and B2.

01:23.610 --> 01:27.670
This is going to be Alerta and layer two is not going to have a squishing function.

01:27.870 --> 01:29.860
It depends on what you're trying to do here.

01:29.880 --> 01:31.810
We are not going to squish the second layer.

01:32.130 --> 01:37.260
So whenever you want to define the model, your model is going to be first.

01:37.260 --> 01:40.350
We are going to do a one on X and then we are going to apply L2.

01:41.110 --> 01:42.240
We don't have a squishing function.

01:42.240 --> 01:45.340
If we did, we would have defined as scooching function over here as well.

01:45.750 --> 01:46.740
So that's all there is to it.

01:47.010 --> 01:52.740
Now what you can do is you can take a random five element vector and pass it to the model and you get

01:52.740 --> 01:55.290
the forward pass in an instance like.

01:56.240 --> 02:02.720
So you get this forward pass over here, you get two values over here, because that five element collective

02:02.840 --> 02:04.370
is going to be multiplied with this.

02:04.670 --> 02:08.840
You are going to get three values out and those are going to be squished.

02:08.960 --> 02:12.650
And then these three values are going to go in and two values are going to come out.

02:12.890 --> 02:15.140
And that is why we have these two values over here.

02:15.170 --> 02:18.560
So if you're not sure about the mathematics, you can go ahead and look at this.

02:18.710 --> 02:23.210
But this is all there is word you are multiplying the matrices together and then adding a vector to

02:23.210 --> 02:23.360
it.

02:23.430 --> 02:25.080
OK, right.

02:25.200 --> 02:28.000
So this code looks slightly messy even now.

02:28.010 --> 02:29.210
So let's go ahead and clean this up.

02:29.420 --> 02:31.940
So our linear layer is going to be a function.

02:31.940 --> 02:33.380
We are going to define it as a function.

02:33.470 --> 02:36.590
So not as this layer is simply a function.

02:36.770 --> 02:43.430
It's a layer which defines the number of nodes or the number of values coming in and the number of values

02:43.430 --> 02:44.030
going out.

02:44.180 --> 02:49.480
OK, there is going to be around end of outcome in notice that this is reversed.

02:49.790 --> 02:52.370
Like I said, five comes in, three goes out.

02:52.580 --> 02:59.000
OK, so we are going to reverse this because for us humans, it's much easier to write the values first

02:59.000 --> 03:02.150
and outproduce that I'm going to run this and then you'll see what this means.

03:02.450 --> 03:08.180
We are going to have B of random values and X is going to be simply this multiplication over here and

03:08.180 --> 03:09.060
this is going to be written.

03:09.320 --> 03:11.450
OK, so let's do this.

03:11.840 --> 03:17.900
The reason this is a function, this is defined as a function is that this function is going to be sent

03:17.900 --> 03:18.230
back.

03:18.410 --> 03:20.870
So linear five comma three is going to be a function.

03:21.200 --> 03:27.320
When you say linear five comma three, it gives you not a value back, but a function that you can later

03:27.320 --> 03:28.310
on call eight.

03:28.400 --> 03:31.160
So if you say linear five three, it gives you a function back.

03:31.670 --> 03:32.010
Right.

03:32.480 --> 03:35.540
And what this is five nodes coming in or going out.

03:35.880 --> 03:39.470
OK, now linear one is going to be linear five comma three.

03:39.500 --> 03:43.730
What this means is our first layer is going to be a linear linear in which five values are going to

03:43.730 --> 03:45.860
come in and three values are going to go up.

03:46.190 --> 03:51.200
The second layer obviously has to have three values coming in and it can have any number of values going

03:51.200 --> 03:51.500
out.

03:51.750 --> 03:56.750
We are defining this as to this three, and this three has to be the same because layer one is going

03:56.750 --> 03:57.740
to connect to layer two.

03:58.100 --> 03:59.650
OK, so let's do that.

04:00.110 --> 04:07.370
So now we have linear linear to the good thing over here is because we have defined these as functions,

04:07.550 --> 04:09.710
we can extract their local values out as well.

04:09.740 --> 04:12.320
So you can say linear under W, it's very easy.

04:12.860 --> 04:17.000
These are called closures, by the way, in case you're interested, when you can access the local variables

04:17.000 --> 04:18.730
of different functions based on this.

04:18.980 --> 04:20.390
So it's not all that important.

04:21.110 --> 04:26.660
Now, what our model is going to be is as before we are going to do the linear on the X and then we

04:26.660 --> 04:30.610
are going to squish it and then we are going to do the linear two over here.

04:31.070 --> 04:31.270
Right.

04:31.330 --> 04:34.620
So now this is the whole code that we have, very clean and nice.

04:35.300 --> 04:36.320
So this is our model.

04:36.530 --> 04:40.700
We can take our dummy data over here and send this to the model.

04:40.700 --> 04:42.290
And this is going to work out perfectly well.

04:42.740 --> 04:49.840
OK, and even easier way to write this can be you do the linear one X, you apply the squishing function.

04:49.880 --> 04:53.070
Whatever value you get out, you pipe it to linear, too.

04:53.480 --> 04:58.820
So this is exactly the same as this, except now this looks a lot more sequential.

04:58.820 --> 05:04.190
So instead of this function composition in which Linear two is written first, but later, it looks

05:04.310 --> 05:05.010
kind of weird.

05:05.390 --> 05:06.960
This is much more easier to read.

05:07.160 --> 05:11.200
So you apply linear one X, you squish it and then you pass it to linear two.

05:11.210 --> 05:15.080
And if you had more layers over here, you could pipe them as.

05:18.020 --> 05:22.280
We need three and so on and so forth, it would make a lot more sense, but for now we have just two

05:22.280 --> 05:23.690
years so we can do that.

05:24.440 --> 05:27.260
We can say Model B of X, exactly the same thing.

05:27.420 --> 05:33.670
OK, now we have defined this linear layer ourselves by flux already comes in with these layers.

05:33.950 --> 05:38.360
We have done this by hand over here so that you understand how easy it is to create your own type of

05:38.360 --> 05:38.720
layers.

05:38.840 --> 05:43.790
Later on, you can go ahead and access everything that is in your neural network using this very simple

05:43.850 --> 05:47.630
index network and look at the layers already provided in flux.

05:47.660 --> 05:51.930
So a couple of examples so we can say using flux, we can go ahead and just restart.

05:52.290 --> 05:57.910
OK, so these are going on clearer, Longport, so that you're sure that we are using this X over here.

05:58.190 --> 06:05.330
So we are using Flux X a little bit of time and then we can say layer one is equal to dence then five

06:05.330 --> 06:10.730
and the activation function is sigma something number of values going in, a number of values going

06:10.730 --> 06:12.380
out and the squishing function.

06:12.740 --> 06:14.240
So we defined this up above.

06:14.390 --> 06:20.300
Here we are using the pre-built lit by this tense has been defined exactly the same way that we have

06:20.300 --> 06:21.390
defined this linear Obadi.

06:21.590 --> 06:24.370
So nothing new, just a little bit easier for us to work with.

06:24.860 --> 06:30.380
We can go ahead and use this index or an easier Syntex would be to use the chain function.

06:30.380 --> 06:33.500
It does exactly the same thing as this guy over here, the pipe symbol.

06:33.890 --> 06:36.320
But again, depends on what you like.

06:36.920 --> 06:38.960
For some, this is much easier to read.

06:39.080 --> 06:40.610
And for some this over here.

06:40.940 --> 06:43.250
This is the common method for flux.

06:43.460 --> 06:44.540
So I'm going to stick with this.

06:44.870 --> 06:45.950
So we change the layers.

06:46.130 --> 06:49.460
The first layer is going to be identically in which ten values are going to come out.

06:49.460 --> 06:50.750
Fi values are going to go out.

06:51.230 --> 06:53.180
OK, so the input shape has to return.

06:53.330 --> 06:54.470
The number of nodes is five.

06:54.470 --> 06:56.450
So five values go out in the next layer.

06:56.450 --> 07:00.860
We are going to have five values coming in and the number of nodes is too small to values go out and

07:00.860 --> 07:04.030
then we apply the softmax, which you would be familiar with.

07:04.040 --> 07:05.110
So this is built in as well.

07:05.870 --> 07:07.270
So we run that.

07:07.280 --> 07:11.060
This is our model and now we can say model two on this random variables.

07:12.250 --> 07:14.320
When you run that, you get a forward pass out.

07:14.740 --> 07:19.420
In this video we have take a look at how to create multiple layers for our dealer network.

07:19.930 --> 07:25.340
In the next video, we are going to take these building blocks and apply them on amnesty.

07:25.720 --> 07:29.230
So amnesty, as you might be aware, is a very common problem in learning machine learning.

07:29.560 --> 07:34.510
And we are going to use the Flux Library to do amnesty classification.

07:34.670 --> 07:35.700
So that's in the next video.
