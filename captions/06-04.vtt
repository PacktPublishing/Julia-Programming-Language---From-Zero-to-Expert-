WEBVTT

00:00.620 --> 00:04.500
In the previous video, we set up the data that has to be fed to the machine learning model.

00:04.640 --> 00:08.690
And here we are going to create the CNN model and everything else that we need to make it work properly.

00:08.720 --> 00:10.220
OK, so first the model.

00:10.430 --> 00:13.300
It's going to be a chain which is going to be a sequential model.

00:13.700 --> 00:19.340
The first layer is a convolution layer in which the filter size is going to be three by three and it's

00:19.340 --> 00:22.920
going to have one channel coming in and 16 channels going out.

00:23.000 --> 00:26.480
OK, and the activation is going to be rectified.

00:26.720 --> 00:29.590
Linear unit, the next layer is going to be a bullet.

00:29.630 --> 00:34.640
So, Max, pulling if you're not familiar with that, you can look at any tutorial for machine learning

00:34.640 --> 00:37.340
and then explain what makes pooling and everything else is.

00:37.640 --> 00:39.020
But really, it's fairly straightforward.

00:39.360 --> 00:44.030
OK, then we have another convolution layer in which 16 channels come in and it go out.

00:44.480 --> 00:47.930
That is followed by a max bullet again, and then we flatten it.

00:47.930 --> 00:52.300
And then there is a dense layer of ten because that is the number of classes that we have.

00:52.520 --> 00:53.700
And finally we have a software.

00:53.720 --> 00:55.010
And finally we have a softmax.

00:55.160 --> 00:56.470
So that is our basic model.

00:56.630 --> 00:58.730
We can run this and we get the model out.

00:58.970 --> 01:02.460
So this looks really weird, but that's our model that we have defined over here.

01:03.020 --> 01:06.510
We are going to need the one called, which is the opposite of one heart.

01:06.570 --> 01:10.400
So if you give it a one shot representation, it's going to create the class for you.

01:11.090 --> 01:15.920
We have the cross entropy, which is the loss, and we also have the throttle, which allows us to perform

01:15.920 --> 01:17.780
some tasks while the learning is going on.

01:17.780 --> 01:18.930
We'll see what this means in a minute.

01:19.310 --> 01:22.050
OK, so we're going to have M is equal to model.

01:22.160 --> 01:24.850
So this is just to make the code look a little cleaner.

01:25.220 --> 01:27.360
We are going to have our optimized as Adam.

01:27.530 --> 01:28.700
There are many other options.

01:28.700 --> 01:31.410
You can use the best gradient descent, you can use the momentum based.

01:31.610 --> 01:35.210
You can use the Nesterov, which is really popular, and you can use the and Adam as well.

01:35.590 --> 01:37.820
We're going to stick with Adama's are optimized for now.

01:38.050 --> 01:40.880
OK, we also need to define what the accuracy is.

01:40.880 --> 01:46.110
So accuracy is going to be, you can word, whatever the model predicts into one goal.

01:46.130 --> 01:50.810
So if this predicted a one out of five, we are going to convert this into a five.

01:50.970 --> 01:56.750
And we are also going to convert our way back to one goal and we are going to do a piecewise equality

01:56.750 --> 01:57.030
check.

01:57.200 --> 02:01.070
So essentially what this means is, on average, how many we get, right?

02:01.520 --> 02:04.010
So that's what accuracy is, how many answers we got.

02:04.010 --> 02:04.310
Right.

02:04.790 --> 02:05.930
So that is our accuracy.

02:06.680 --> 02:08.930
Our loss is going to be flux, not losses.

02:08.930 --> 02:09.760
Don't cross entropy.

02:10.250 --> 02:16.400
What the model predicted and what the ground truth was that Harless gets across entropies defined previously

02:16.400 --> 02:21.100
in the previous video or the one before that we defined our loss manually.

02:21.140 --> 02:26.090
Here we can use just across entropy that comes with flux so we don't have to define it by hand.

02:26.330 --> 02:27.530
OK, so that's a loss.

02:28.220 --> 02:35.480
We can go ahead and perform the training using the loss and the parameters and the training set and

02:35.480 --> 02:38.320
the optimizer so this we can do before given.

02:38.660 --> 02:43.880
So instead of having to calculate different matrices by hand, after a little while, we can give it

02:43.880 --> 02:44.650
a callback.

02:44.870 --> 02:50.810
So what a callback function does is it's going to be called automatically by the training function after

02:50.810 --> 02:51.420
give an interval.

02:51.590 --> 02:54.170
So for that, we are going to define a new function if LCB.

02:54.290 --> 02:57.530
So this is over here and we're going to use it to throttle.

02:57.560 --> 03:03.740
So what does is at this many seconds, it's going to call this function for you.

03:04.010 --> 03:05.180
So that's what it does.

03:05.460 --> 03:08.090
It's going to ask you for a function and a time.

03:08.420 --> 03:11.480
So every 10 seconds it's going to call this function.

03:11.660 --> 03:13.310
And what is this function doing?

03:13.550 --> 03:20.210
It's not taking any parameter in and it's simply going to show the accuracy on the test set at that

03:20.210 --> 03:20.580
time.

03:20.600 --> 03:26.450
So if you recall, we created this test set and it's going to every 10 seconds, just show the accuracy

03:26.450 --> 03:28.010
on the test set at that time.

03:28.240 --> 03:29.520
OK, so this makes sense.

03:30.200 --> 03:33.110
So once again, we defined the loss as cross entropy.

03:33.500 --> 03:39.980
We have the parameters for Model M, which are going to be built automatically because we have defined

03:39.980 --> 03:42.200
this thing over here so we don't have to do it by hand.

03:42.530 --> 03:43.490
We have the training set.

03:43.880 --> 03:47.300
We have the optimizer, which is currently said to Adam, but you can change it.

03:47.600 --> 03:51.920
And we have the callback function, which is going to be automatically called every ten seconds to show

03:51.920 --> 03:52.550
the accuracy.

03:52.850 --> 03:59.810
OK, so if you do that, it's going to run the training and every 10 seconds or so it's going to output

03:59.810 --> 04:01.750
the accuracy on our test set.

04:02.120 --> 04:03.860
So let's see how well this does.

04:04.070 --> 04:07.540
So it starts off with a really good accuracy of point of five.

04:07.550 --> 04:08.540
So that's really horrible.

04:08.840 --> 04:12.790
But immediately after a little while, it jumps two point six four.

04:13.130 --> 04:15.140
So you can go ahead and run this again.

04:15.560 --> 04:19.460
So if you run this again, it's going to start off from where you left off.

04:19.460 --> 04:25.250
So point six goes two point seven two, and then it's going to improve and it goes all the way up to

04:25.250 --> 04:28.760
approximately 90 percent accuracy, which is fairly good for a start.

04:29.030 --> 04:34.460
OK, it might look slightly complicated at the moment, but what we're going to do now is clean everything

04:34.460 --> 04:40.280
up, get rid of everything that was here for explanation and make the code look really clean so that

04:40.280 --> 04:43.640
you can see what you actually have to do when using flux.

04:43.700 --> 04:45.050
OK, so let's go ahead and do that.

04:45.440 --> 04:46.490
So we're going to.

04:47.650 --> 04:52.900
Restart the gun and clear all the airports so that you know that this is working perfectly fine and

04:52.900 --> 04:55.030
fresh, so this is all we have.

04:55.210 --> 04:56.200
We have some imports.

04:56.890 --> 05:01.520
We have the function that gets the training and test data and we pass it.

05:01.550 --> 05:04.960
The bad guys, it goes ahead and loads the mislabel.

05:05.590 --> 05:11.560
It loads the images, converts them to the proper format and creates a training site based on the partitions

05:11.560 --> 05:13.640
that we have according to the bad sites.

05:13.840 --> 05:15.720
So that's the training images and training limits.

05:16.030 --> 05:18.370
We also have the test images and it's levels.

05:18.370 --> 05:21.560
So we do the same thing over here and then return all of these back.

05:21.820 --> 05:25.830
So that's our function for getting the data for our model.

05:25.870 --> 05:28.430
We have a simple function over here that creates the model.

05:28.750 --> 05:32.590
This is just separated so that we can look at it and change it later on.

05:33.400 --> 05:38.530
Finally, we have a trained model function which asks you how many iterations you should run and it

05:38.530 --> 05:41.730
also has a default parameter for optimizer.

05:41.830 --> 05:44.990
Currently we have set this to Adam, but you can posit a different one.

05:46.000 --> 05:48.390
We are going to do a musical to build model.

05:48.910 --> 05:51.790
We are going to get all the training and test data.

05:53.130 --> 05:57.810
We are going to set the laws, we are going to define what accuracy means, we are going to set our

05:57.810 --> 06:03.390
call back and then go ahead and do the training for a number of times.

06:03.630 --> 06:05.230
OK, so that's all there is to it.

06:05.520 --> 06:11.130
Now, you can go ahead and call this on the data with the optimizers said to Adam and patrician's is

06:11.130 --> 06:11.700
equal to three.

06:12.030 --> 06:13.080
So you can just run that.

06:13.590 --> 06:18.210
It takes a little bit of time, but that's because Julia has to do a lot of compilation the first time

06:18.210 --> 06:20.340
around the next time it runs really fast.

06:20.460 --> 06:25.620
So as you can see, it starts off with a pure accuracy of point one eight, and it's going to jump to

06:25.620 --> 06:30.150
a really high accuracy pretty quickly after it finishes with the accuracy of point eight eight.

06:30.510 --> 06:34.140
So 88 percent accuracy, not bad, but not really state of the art.

06:34.320 --> 06:36.860
But we've only run the hydrogen for a little while.

06:37.170 --> 06:42.840
So as you can see, the whole code that we have is very clean and it's a very small piece of code and

06:42.840 --> 06:43.520
it works really well.

06:43.710 --> 06:48.930
OK, and the idea is that we can now go ahead and experiment using different optimizers, different

06:48.930 --> 06:53.700
number of iterations and different other options so we can go ahead and run this for an which works

06:53.700 --> 06:54.120
really well.

06:54.430 --> 07:01.980
We can also run it for Adam and the base gradient descent algorithm, as well as a gradient descent

07:01.980 --> 07:05.420
algorithm which has been set to learn a little faster.

07:05.430 --> 07:11.070
So alpha value, the learning parameter is set to zero point for all of these and wait until all of

07:11.100 --> 07:14.850
these are done and then we can discuss briefly their results.

07:15.150 --> 07:22.290
So all of these have finished running and we can see that Adam topped out at 88 percent and Adam went

07:22.290 --> 07:27.120
right up to ninety one point six percent of them did really poorly, like 66 percent.

07:27.120 --> 07:32.010
Only the base gradient descent algorithm went all the way up to ninety one point eight percent.

07:32.010 --> 07:38.670
And the gradient descent algorithm with the learning rate Alpha set two point four, went really quickly

07:38.670 --> 07:40.250
to appoint nine five nine.

07:40.440 --> 07:41.820
So this works really well.

07:41.850 --> 07:47.310
So this goes to show that even the state of the art models may not perform well if you don't set them

07:47.310 --> 07:47.730
properly.

07:47.730 --> 07:48.060
So.

07:49.010 --> 07:55.760
The base like 2012, or I guess even older than that gradient descent algorithm works really well if

07:55.760 --> 07:57.980
you set the alpha parameter properly.

07:58.310 --> 08:01.760
So the point is that you can go ahead and experiment with all of these different things.

08:01.970 --> 08:04.550
And if you want to look at other stuff, you can click on this link.

08:04.760 --> 08:09.920
And that will take you to the flux documentation where you can take a look at different optimises and

08:09.920 --> 08:11.840
their respective parameters as well.

08:11.870 --> 08:17.030
So a lot of stuff to go over here, but you can see that you have the descent algorithm that we've used

08:17.150 --> 08:22.110
in which you can set the alpha, which is the learning rate so integrated into it.

08:22.160 --> 08:22.940
They call it Eita.

08:22.940 --> 08:24.220
We typically called Alpha.

08:25.100 --> 08:30.590
You can also have the Momentum Optimizer and Nazarov and so on and so forth.

08:30.590 --> 08:32.310
Quite a lot of them are available over here.

08:32.540 --> 08:36.440
So this documentation is typically in transition, so it's difficult to work with.

08:36.650 --> 08:40.520
But once you understand this code over here, you should be able to read it really well.

08:40.940 --> 08:45.070
So that concludes our discussion about how to use machine learning algorithms.

08:45.080 --> 08:51.020
All you have to do now is go ahead and change your model over here and experiment further with this.

08:51.290 --> 08:56.480
In the next video, we are going to see how you can get your experiment from one machine to another

08:56.480 --> 09:02.240
machine or continue an experiment that you were getting out earlier on by saving your model and loading

09:02.240 --> 09:02.750
it later on.
