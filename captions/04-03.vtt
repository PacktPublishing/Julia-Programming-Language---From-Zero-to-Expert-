WEBVTT

00:00.750 --> 00:02.860
Let's go ahead and do some further data manipulation.

00:03.120 --> 00:09.510
We are going to load the data again from a ACSU file, but this time we are going to try and save it

00:09.510 --> 00:12.180
into a much better format, which is a Pajero.

00:12.420 --> 00:13.770
So we'll get to that in a minute.

00:13.870 --> 00:16.350
But let's take a look at the data that we are starting with.

00:16.360 --> 00:18.420
So this is the auto data set.

00:18.660 --> 00:24.150
So it has information about different cars, their mileage and all of that stuff.

00:24.150 --> 00:24.370
Right.

00:24.390 --> 00:27.570
So we can download this data from this.

00:27.570 --> 00:33.180
You are using the built in download function that Julia comes with and we can save it in data.

00:34.590 --> 00:39.270
I've already done this or I'm not going to run this cell, but we can then go ahead and read all the

00:39.270 --> 00:43.350
lines of this file and see what it looks like just to see if it's there.

00:43.480 --> 00:45.140
OK, so this is what it looks like.

00:45.690 --> 00:48.450
So this is a building function, so works out of the box.

00:48.900 --> 00:53.550
Now we're going to try and use the CSV and data frames package, just as we did in the previous video

00:53.550 --> 00:54.420
and see if this works.

00:54.990 --> 00:57.240
So we do the exact same thing.

00:57.240 --> 00:59.730
We give it the headers which are not present in the CEV.

00:59.730 --> 01:08.290
So we are going to hardcore them over here and we are going to convert the missing values to end any.

01:08.510 --> 01:12.800
OK, and then we pipe this to the data frame, just as we did previously.

01:13.200 --> 01:15.980
So when we do that, you'll see that we'll get an error.

01:16.440 --> 01:23.730
And the reason for that is that there are some TAB characters over here and then some non standard formatting

01:23.730 --> 01:25.960
going on because of which we are getting some errors.

01:25.970 --> 01:27.000
So here's a warning.

01:27.330 --> 01:32.940
Borst expected nine columns because I give it nine columns over here, but didn't reach the end of line

01:32.940 --> 01:33.740
around the data.

01:33.750 --> 01:33.970
Right.

01:34.000 --> 01:38.100
So what it's saying is it got to the end, but nine columns were not filled.

01:38.130 --> 01:40.140
And the reason for that is this deal here.

01:40.290 --> 01:41.760
If you can see I'm going to zoom in.

01:42.690 --> 01:45.390
So this guy over here, this is causing the problem.

01:45.780 --> 01:51.600
And this is a very common problem when you're working with CSP files, there might be some hidden characters

01:51.600 --> 01:54.510
and stuff going on that is causing problems for your data set.

01:54.870 --> 01:58.710
So what we're going to do is we're going to see how we can manage this in our code.

01:59.250 --> 02:02.780
So let's go ahead and first read this whole thing into a string.

02:02.820 --> 02:09.210
So we are going to read this file as a string into our underscore start variable.

02:09.300 --> 02:15.960
OK, then we're going to go ahead and replace all the characters with the space and that's going to

02:15.960 --> 02:17.550
essentially clean it up just a little bit.

02:17.820 --> 02:19.770
And then we are going to create an IO buffer.

02:19.950 --> 02:25.790
So our buffer is essentially a buffer in which you put a string and you can use that as an input output

02:25.800 --> 02:26.340
or a file.

02:26.380 --> 02:28.730
OK, so you'll see what this means in a minute.

02:28.980 --> 02:33.780
So we create a new buffer based on this string that we just created and then we are going to do the

02:33.780 --> 02:38.580
exact same thing with this ESV and data frame, except this time we're going to give it the buffer as

02:38.580 --> 02:40.380
the source instead of the file.

02:40.600 --> 02:47.700
OK, so this time because we've converted all the Ts to spaces and we are setting the delimiter as a

02:47.700 --> 02:50.020
space character, this works out perfectly well.

02:50.190 --> 02:52.650
So now it can read all line columns and everything works.

02:52.920 --> 02:53.080
Right?

02:53.190 --> 03:00.690
So there are many, many things like this in data science that you come across slowly and steadily when

03:00.690 --> 03:02.430
you do some exploration on your own.

03:03.210 --> 03:05.850
I just want to cover one use case, which is very common.

03:06.040 --> 03:11.220
OK, now we have this red and we want to essentially figure out how many values are missing.

03:11.700 --> 03:12.710
So some values are missing.

03:12.720 --> 03:13.890
You can see a couple of over here.

03:14.260 --> 03:16.140
So we want to figure out how many are missing.

03:16.470 --> 03:24.780
So what we can say is we can some count missing for each column in D.F., so for each call.

03:25.590 --> 03:32.310
So forkball in each corridor that loops over the columns and we are going to count is missing of the

03:32.670 --> 03:33.790
column that we are measuring.

03:33.810 --> 03:37.350
So essentially are some of the missing values in all columns.

03:37.510 --> 03:39.390
OK, this looks really complicated.

03:39.600 --> 03:41.250
We have a much cleaner method of doing this.

03:41.530 --> 03:47.220
We can convert the data frame into a matrix just as we did previously, and then just county is missing.

03:47.460 --> 03:50.190
So that is much cleaner way of doing the exact same thing.

03:50.850 --> 03:53.040
We can also go ahead and map calls.

03:53.040 --> 03:59.250
So we are going to take D.F. and map cards, extract columns and passes them to this function over here.

03:59.730 --> 04:05.100
And each column is going to turn into X and we are going to count is missing and then map calls is going

04:05.100 --> 04:09.030
to return the individual counts of missing values.

04:09.360 --> 04:17.040
So MPG is missing eight values, horsepower is missing six values, and all the other columns have no

04:17.040 --> 04:17.790
missing values.

04:18.150 --> 04:21.630
So depends on what kind of analysis you're doing.

04:21.630 --> 04:24.660
You might be interested in doing this or this or this.

04:25.080 --> 04:29.940
If you are interested in finding optimizing values according to the column, you use the third method.

04:30.240 --> 04:36.080
OK, now let's wait and do some more interesting stuff, such as adding a column based on data.

04:36.300 --> 04:42.530
So previously we added the ID column, which was just going from one to the number of values here.

04:42.540 --> 04:48.020
What we are trying to do is we are going to extract the brand name from the name of the car.

04:48.180 --> 04:48.440
Right.

04:48.490 --> 04:56.250
For instance, we have the name of the car over here as, for instance, Buick and or Chrysler or Ford.

04:56.460 --> 05:00.180
We want to extract this because this is the brand and then we have the car name over here, so.

05:00.240 --> 05:04.800
You want to extract this word Ford and this Toyota or Dodge or Chevrolet.

05:05.070 --> 05:11.020
OK, now the way to do this is we are going to first see what name looks like.

05:11.040 --> 05:12.690
So the name is this guy over here.

05:12.720 --> 05:13.620
This is what it looks like.

05:13.870 --> 05:16.730
We want to split it on a space character.

05:16.740 --> 05:22.980
So if you do that, you can say split, dot, dot, dot name and we need dot because we want to broadcast

05:22.980 --> 05:27.180
the split function over all the different elements within the series.

05:27.660 --> 05:33.270
If you don't broadcast it, it's not going to it's not going to go into the individual element and it

05:33.270 --> 05:33.780
won't work.

05:34.140 --> 05:39.360
But because we have a broadcast, it goes ahead and apply split individually to the first string and

05:39.360 --> 05:40.740
the second string and the third string.

05:41.340 --> 05:46.260
Once we have that, we can apply the first function, which returns the first element from the array.

05:46.530 --> 05:48.600
And we are going to broadcast this as well.

05:48.810 --> 05:51.720
OK, so what brand is this here?

05:52.140 --> 05:56.210
So first you split it, then you get the first element out.

05:56.220 --> 05:57.220
So that gives you the brand.

05:57.640 --> 06:02.130
OK, we also put that as a new series within the data frame.

06:02.130 --> 06:03.000
So that goes in.

06:03.330 --> 06:07.940
So now we can go ahead and retrieve some of these values so we can retrieve and brand.

06:07.950 --> 06:11.220
So name is this guy over here and produce this guy over here.

06:11.310 --> 06:13.260
OK, and we are seeing the first 10 columns.

06:13.770 --> 06:17.460
If you see the size of D.F., it has four or six rows.

06:17.880 --> 06:20.850
We can go ahead and drop all the missing values just as we did before.

06:21.190 --> 06:27.030
You can say data is equal to drop missing D.F. And this is going to give us a slightly smaller data

06:27.030 --> 06:27.480
frame now.

06:28.870 --> 06:30.400
The 14 missing values are now gone.

06:30.670 --> 06:36.520
OK, now if we try to count the missing values from D.F. two, obviously there are no missing values

06:36.520 --> 06:37.450
because we just dropped them.

06:38.380 --> 06:43.840
One of the most common things in data science is to extract certain rules based on the criteria.

06:44.020 --> 06:48.520
For instance, if you are interested in getting just those rules which have the brand Assab, we can

06:48.520 --> 06:55.150
say, do you have to leave to dot, brand, dot, dot equal to which means essentially apply broadcast

06:55.300 --> 06:58.300
of equal to function on each individual row.

06:58.870 --> 07:02.620
And we are saying if that equals sub and done all the columns.

07:02.890 --> 07:05.020
So that's a slightly verbose way of doing this.

07:05.320 --> 07:07.270
But this is what people typically do with.

07:08.050 --> 07:12.610
So you do that and you get all the values which have Saab as the brand.

07:13.210 --> 07:16.420
But a much better way of doing this would be using the filter method.

07:16.420 --> 07:20.970
And you will say that filter takes a criteria and it source.

07:21.160 --> 07:24.010
We did this when we were looking at functional programming.

07:24.010 --> 07:26.310
So functional programming 030 do over here.

07:26.320 --> 07:27.120
We saw this there.

07:27.970 --> 07:31.360
If you don't recall this you can go ahead and take a look at that again.

07:31.780 --> 07:38.650
But here we are going to pass this function to filter and what we're saying is only keep those rules

07:38.650 --> 07:40.450
which satisfy this criteria.

07:40.780 --> 07:46.000
So we give it a rock and roll brand when it is equal to Saab, we're going to keep it right.

07:46.030 --> 07:47.890
So this retains the same value for us.

07:48.070 --> 07:52.680
But you will notice that this is a lot more readable than this guy over here.

07:52.990 --> 07:58.000
So it's typically recommended that you use filter for doing filtering because it makes more sense.

07:58.260 --> 08:03.700
OK, finally, the actual issue that we started with, we want to load the data.

08:03.970 --> 08:09.640
We want to clean it up a little bit, and then we want to write it into another file so that the next

08:09.640 --> 08:11.610
time we read it, we don't have to do the cleaning again.

08:11.920 --> 08:17.500
So we are going to simply use the computer to function, to write our data in order to clean NSV.

08:17.890 --> 08:22.930
So that's done so that Autodesk cleaned out CSFI.

08:23.200 --> 08:29.440
So you will see that this is perfectly clean now and you can take a look at the names and Granzow brand

08:29.440 --> 08:30.440
has been added over here.

08:30.910 --> 08:33.670
However, CSFI is not very efficient.

08:34.240 --> 08:40.660
We have a new highly recommended format for storing very large data, which is about addle.

08:40.930 --> 08:45.910
So you can go ahead and take a look at this link and it'll tell you the benefits of erro.

08:46.390 --> 08:50.290
Very briefly, you can use Addle for very large datasets.

08:50.290 --> 08:55.330
For instance, if you have 64 gigabytes of data and you want to load it into a sixteen gigabyte RAM

08:55.330 --> 08:57.070
system, you can easily do that.

08:57.070 --> 09:01.260
Using Arrow and Arrow is going to essentially manage all the swapping and swapping out for you.

09:01.600 --> 09:06.910
So you don't have to worry about loading the required data into RAM because it becomes very problematic

09:06.910 --> 09:07.510
very quickly.

09:07.960 --> 09:14.290
So it's typically recommended that you go with Adam and you are doing any data science with very large

09:14.290 --> 09:16.450
data, which is the whole point of data science.

09:16.520 --> 09:21.540
Julia, to be frank, anyway, so for that we have a package in Julia called Arrow.

09:21.550 --> 09:25.280
So we are going to say using Arrow, if you don't have it installed, you can install it using this

09:25.280 --> 09:25.870
cell over here.

09:26.320 --> 09:29.860
And then we are going to write the profile using Arrow Dot.

09:29.860 --> 09:30.160
Right.

09:30.310 --> 09:33.090
And we are going to give it the name over here.

09:33.460 --> 09:36.280
Now, you will notice that this guy is.

09:37.270 --> 09:42.070
Not something that we can open and the reason for that is Addo is a binary format.

09:42.080 --> 09:44.800
It's not a text format, not a character format.

09:44.800 --> 09:49.570
So you cannot read it directly over here, but we can read it and you're going to read it just for the

09:49.570 --> 09:51.570
sake of our understanding over here.

09:51.760 --> 09:57.400
OK, so do you have to go to another table and we are going to give it the source and then we are going

09:57.400 --> 10:01.560
to convert it into a data frame using deep pipes index, just as we did with CSFI.

10:01.840 --> 10:04.920
So once you have a narrow file, this is all you need to do to load it.

10:05.170 --> 10:10.000
And because it's binary, it's very unlikely that it's going to have the problems that we had it.

10:10.900 --> 10:12.230
We can go ahead and do some grouping.

10:12.640 --> 10:18.010
So once you've read this and you have the same thing back in a data frame, you can go ahead and do

10:18.010 --> 10:18.520
some groping.

10:18.520 --> 10:22.950
So, for instance, you can grow by the brand who can say group round is equal to gloopy.

10:23.350 --> 10:29.440
So this is a source that we want to group and this is the criteria or the cell on which we are going

10:29.440 --> 10:30.170
to group.

10:30.550 --> 10:34.230
So we do this and group round is going to have some groups.

10:34.230 --> 10:37.930
The first group is where Brand is to Cheverly and these are the guys over here.

10:38.440 --> 10:43.180
So what we're trying to do over here in this case study is and this is the Nissan one, what we're trying

10:43.180 --> 10:50.860
to do over here is take a look at how many cars, for instance, are created by Ford, how many by Nissan

10:50.860 --> 10:51.770
and so on and so forth.

10:51.790 --> 10:52.010
Right.

10:52.450 --> 10:58.540
So, for instance, if you take a look at the group trends and we get just the Ford values out, these

10:58.540 --> 11:04.270
are the cars which are created by Ford, OK, because we group that and now we are giving this notice

11:04.270 --> 11:05.190
this come over here.

11:05.410 --> 11:06.550
This is very important.

11:06.550 --> 11:08.620
If you don't do this, you're going to get an error.

11:09.580 --> 11:11.710
And the reason for that is this.

11:12.130 --> 11:15.460
This list over here has to be a dual collection.

11:15.880 --> 11:20.970
And if you don't put a comma over here, it's simply a string which is surrounded by braggarts.

11:21.280 --> 11:22.090
We don't want that.

11:22.600 --> 11:23.980
We want a tuple.

11:24.250 --> 11:28.720
So this converts it from a string sound right backwards to a tuple.

11:28.990 --> 11:31.480
OK, so it's very important that you understand this.

11:31.660 --> 11:34.180
You can give it other grouping criteria's as well.

11:34.180 --> 11:36.790
But for this simple case study, this is enough.

11:37.090 --> 11:39.030
OK, so these are the four things.

11:39.400 --> 11:43.990
Now we can use the statistics package to, for instance, calculate mean and all the other statistics

11:43.990 --> 11:44.320
as well.

11:45.100 --> 11:50.610
Let's go ahead and calculate the MPG statistics by using the combine function.

11:50.890 --> 11:56.260
So we have the group grants that we created over here and we are going to combine it and calculate the

11:56.260 --> 11:58.350
mean of the MPG rate.

11:58.420 --> 12:02.950
So because we have the grouping, the results are going to be grouped accordingly.

12:03.190 --> 12:06.180
So Chevrolet has this mileage and so on and so forth.

12:06.580 --> 12:09.010
Now we can go ahead and sort them even.

12:09.250 --> 12:11.680
But before that, notice a couple of things.

12:11.890 --> 12:17.470
Number one, this is too much data and I don't like it so we can change the energy lines 10.

12:17.740 --> 12:20.860
And that is going to just show that 10 rows over here.

12:21.220 --> 12:24.470
What this function does is it takes the mean column from each group.

12:24.490 --> 12:28.680
So this MBG column from each group passes it to the mean function.

12:28.690 --> 12:34.550
This function would here and then it creates a group name to aggregate function mapping.

12:34.570 --> 12:38.750
So this is the group name Cheverly and the aggregate mapping is this guy over here.

12:38.770 --> 12:43.240
So this is the mean MBG and it's going to mean MBG comes from over here and means this function.

12:43.460 --> 12:47.440
OK, we can also posit any function using the anonymous functions syntax.

12:47.500 --> 12:48.490
We'll see that in a minute.

12:49.330 --> 12:54.140
We can do sorting by using the bank functions so we can give it Brandenberg.

12:54.730 --> 13:01.200
This is the criteria for starting and we want to do a in-place reverse or descending order sort.

13:01.690 --> 13:04.930
So these are the people who have the highest mileage.

13:04.960 --> 13:08.380
So just to recap, what we did was we loaded the data.

13:08.650 --> 13:10.810
We combine it using some.

13:11.720 --> 13:15.750
Aggregate and column, and then we sorted it, that's all we've done.

13:16.220 --> 13:21.190
Now let's go ahead and take a look at one of the more powerful features of Julia.

13:21.350 --> 13:24.470
You will notice that we have the same brand MBG being repeated.

13:24.470 --> 13:26.150
So Brandenberg Group brands.

13:27.030 --> 13:30.840
And then Brandenberg, again, if you change the variable name in one place, you have to change it

13:30.840 --> 13:33.170
in all places so it becomes very messy very quickly.

13:33.420 --> 13:35.850
We want to see how we can do this really easily.

13:36.060 --> 13:39.270
So, for instance, you have a Autogen column over here.

13:39.280 --> 13:44.260
So what Origin says is, is it from us, from Europe or from Japan?

13:44.280 --> 13:45.300
So one is for us.

13:45.300 --> 13:46.830
Two is for Europe and three for Japan.

13:47.030 --> 13:49.350
OK, so that is the semantics of this column.

13:50.040 --> 13:54.540
What we want to do is we want to figure out different statistics based on this origin.

13:54.570 --> 13:59.880
OK, so first we are going to group by brand and then figure out if each brand actually belongs to one

13:59.880 --> 14:02.060
origin because that's how it should be.

14:02.340 --> 14:08.610
If you have, for instance, Chevrolet going into the origin as us and as Japan, then there is a problem

14:08.610 --> 14:09.130
with our data.

14:09.150 --> 14:12.220
So this is just to make sure that our data is working perfectly fine.

14:12.420 --> 14:15.090
What we're more interested in right now is this index.

14:15.390 --> 14:17.410
So you have the group over here.

14:17.430 --> 14:18.620
So this is fairly straightforward.

14:18.840 --> 14:25.140
Then we want to combine this data frame, but the criteria is going to be Oregon and Oregon is going

14:25.140 --> 14:30.050
to be not mean, but the length of the unique values for X.

14:30.300 --> 14:40.170
So what this is saying is take the X or the role or the origin, do a unique and then see how many there

14:40.170 --> 14:40.370
are.

14:40.530 --> 14:40.800
Right.

14:40.830 --> 14:43.420
So so let's go ahead and run this and you'll see what this means.

14:43.740 --> 14:48.090
So Cheverly only one origin, Buick only one origin.

14:48.090 --> 14:50.730
So all of them have just one origin, and that makes sense.

14:50.760 --> 14:54.630
OK, so let's make this a little prettier, because this is really problematic.

14:54.630 --> 14:57.660
As you can see, it's not very easy to understand what's going on.

14:58.080 --> 15:00.450
So that's the whole point of this this section of the video.

15:00.750 --> 15:02.570
It's difficult to understand what's going on.

15:02.760 --> 15:05.190
That's why I didn't clean the code using the pipe operator.

15:05.910 --> 15:11.130
And we are going to if you don't have the package installed, you should start started using this index.

15:11.670 --> 15:14.220
What we're going to do is we're going to do an add pipe.

15:14.430 --> 15:20.360
What this macro does is it allows us to use this underscore and I'll explain what that means in a minute.

15:20.610 --> 15:23.910
So you take the data frame, you pipe it to the group function.

15:24.390 --> 15:29.900
And whatever the result of this thing was, it is put in the underscore location, right.

15:29.910 --> 15:31.320
So that the plane was loaded.

15:31.800 --> 15:38.280
That was the output of this line that goes over here, then grew by Volks on that and groups it based

15:38.280 --> 15:43.000
on brand, whatever the output of this whole thing is that goes into this underscore.

15:43.200 --> 15:44.900
So now this is going to be a data frame.

15:44.910 --> 15:45.930
I only have to give it a name.

15:45.930 --> 15:53.490
Now we are going to combine this origin and the account of the unique values for the origin over here.

15:53.800 --> 15:59.700
OK, so this makes Oracle look a lot cleaner and once you have this, you get the same result out.

15:59.730 --> 16:01.030
But the court is much cleaner.

16:01.770 --> 16:07.760
What the underscore does is takes the previous lines outward and put it in place of the underscore.

16:08.460 --> 16:10.620
Then all of this goes over here.

16:11.100 --> 16:14.850
For instance, if you want to sort it, you can add another line over here.

16:14.850 --> 16:16.960
And that is going to Saajid will do that in just a minute.

16:17.850 --> 16:22.630
So you can calculate the extreme just to make sure that all the values are what they are.

16:22.650 --> 16:23.230
So we're fine.

16:23.730 --> 16:27.840
So going back to the starting example, you can do the same thing as we did before.

16:28.170 --> 16:33.720
So, for instance, you can have data from going into group and we are going to group not just by brand,

16:33.720 --> 16:35.040
but by origin and brand.

16:36.090 --> 16:41.760
And then we are going to combine whatever the result of this was based on the number of rows.

16:42.450 --> 16:46.820
So what this is going to say is how many cars are in that brand?

16:46.950 --> 16:47.290
OK.

16:47.700 --> 16:49.140
And then we are going to sort it.

16:50.640 --> 16:56.820
By passing by passing the output of this guy to the search function over here, it's a.

16:57.860 --> 17:06.020
Take the data from Grubert, combine it and calculate the number of rows and then sorted in descending

17:06.020 --> 17:08.480
order based on the number of rows, so.

17:10.100 --> 17:17.150
Ford has 53 data points in this data that we have, Chevrolet has 44, has 32 and so on and so forth.

17:17.390 --> 17:21.890
OK, this is what is called a long format.

17:21.890 --> 17:23.560
So it's increasing downwards.

17:23.900 --> 17:26.710
Sometimes we might want to convert it into a wide format.

17:26.720 --> 17:29.650
If you're not really sure what that is, don't worry about it.

17:29.660 --> 17:31.040
You can skip the rest of the video.

17:31.220 --> 17:36.950
But if you are aware of what device, format versus long format is, you can go from long format to

17:36.950 --> 17:38.900
white format using the unstaffed function.

17:39.170 --> 17:42.710
And you'll see what the difference between these two is if you just run it.

17:42.920 --> 17:49.820
OK, so what unstrapping does is it takes Ford and essentially creates a multiple column values over

17:49.820 --> 17:50.000
here.

17:50.010 --> 17:52.220
So all the ones are going to go over here.

17:52.250 --> 17:58.040
So fifty three over here and all the threes, for instance, for Toyota, they are going to go over

17:58.040 --> 17:58.330
here.

17:58.700 --> 17:59.030
So.

18:00.080 --> 18:04.170
Essentially, it creates a separate column for each individual value over here.

18:04.190 --> 18:05.350
So this is called divide format.

18:06.110 --> 18:10.870
Sometimes it's much easier to work with and you see all of these missing values.

18:10.880 --> 18:16.460
We can get rid of them by converting them into zeros, using the coalesce function, which is broadcasted

18:16.670 --> 18:17.550
to the whole different.

18:17.690 --> 18:20.390
And so when you do that, you get these values out.

18:20.420 --> 18:27.770
So what this is saying is Ford has the origin as one point zero in 53 cars, which makes sense.

18:28.100 --> 18:30.450
Chevrolet has forty four in one point zero.

18:30.470 --> 18:33.910
So all of these are US cars, all of these are Japanese cars.

18:34.160 --> 18:37.140
And this guy over here is the European one.

18:37.480 --> 18:43.460
So so the whole point of this video has been to do different bits of manipulation with data.

18:43.460 --> 18:47.600
As you might be aware, data science is a huge field and there are a huge number of things that you

18:47.600 --> 18:47.930
can do.

18:48.290 --> 18:54.380
What we want to do over here was get a feel of how Julia approaches the different problems and how it

18:54.380 --> 18:59.680
does the structuring of the code, along with some of the syntax details of Julia.

18:59.900 --> 19:04.010
So hopefully after this, whenever you look up a Julia tutorial, you would be able to understand what

19:04.010 --> 19:08.890
it does and what the syntax is and what the semantics are behind the different opportunities.

19:08.930 --> 19:13.970
In the next video, we are going to apply a particular problem of clustering based on map data and see

19:13.970 --> 19:17.870
how we can read a simple dataset and do some clustering based on that.
