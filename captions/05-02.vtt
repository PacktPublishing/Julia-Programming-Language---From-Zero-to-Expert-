WEBVTT

00:00.770 --> 00:05.030
Let's go and apply traditional machine learning to a classification problem on an existing data.

00:05.480 --> 00:10.820
This will just give you an idea of how the built in laboratories for classification work and Julia.

00:11.090 --> 00:15.830
So let's go ahead and use the other datasets and MLB packages.

00:16.370 --> 00:19.130
If you don't have them installed, you can install them using these two cells.

00:19.130 --> 00:19.810
I already do.

00:20.090 --> 00:21.820
So I'm going to just start using them.

00:22.040 --> 00:28.460
So our data sets provides the information about well-known datasets coming from our and base is the

00:28.490 --> 00:30.230
traditional machine learning collection.

00:30.950 --> 00:35.900
So let's go ahead and load the iris data set, which is the information about different flowers and

00:35.900 --> 00:38.580
their species into our iris field.

00:38.600 --> 00:39.440
So there you go.

00:40.190 --> 00:45.080
We can go ahead and take a look at what these dataset looks like in this table that will show up in

00:45.080 --> 00:45.640
just a minute.

00:45.830 --> 00:48.620
So we have sibilant, separate, petulant preterit.

00:48.620 --> 00:52.040
And in the species, you might have seen this before as well.

00:52.250 --> 00:57.220
Let's go ahead and see what the actual pipeline looks like for doing machine learning on this data set.

00:57.470 --> 01:00.440
So we're first going to convert this into a matrix.

01:00.450 --> 01:05.360
So all the rows and only the first four columns, which are the features.

01:05.480 --> 01:09.740
So those are going to go into X and the labels are going to come from the fifth column.

01:09.890 --> 01:10.720
So there we go.

01:11.060 --> 01:12.350
These are the columns that we have.

01:12.560 --> 01:17.400
So little map can tell us what the actual values correspond to the actual labels.

01:17.690 --> 01:21.880
So one is to is regular and three is Virginica.

01:22.460 --> 01:28.700
We can go ahead and use the label encode function to convert all the labels into their corresponding

01:29.270 --> 01:30.220
numeric values.

01:30.230 --> 01:31.160
One, two and three.

01:31.370 --> 01:35.210
OK, so once we do that, we're going to be converted into these values.

01:35.990 --> 01:40.070
OK, so we are going to now create the training and testing splits.

01:40.310 --> 01:45.320
So essentially what we are going to do is we are going to do a class split, which means for each class

01:45.320 --> 01:49.940
we are going to take like 70 percent values and put them in the training and the rest of them we are

01:49.940 --> 01:51.440
going to put into testing.

01:51.740 --> 01:56.870
So the call is going to be like this or Berglas, Plett, we are going to give it away and we are going

01:56.870 --> 02:01.210
to say that point seven of the values should go into the trainings.

02:01.490 --> 02:06.990
So this is going to return the IDs from the dataset, which should be put into the training side.

02:07.010 --> 02:09.460
So let's go ahead and do that and then it'll make sense.

02:09.740 --> 02:17.330
So we're going to do using random because we want to have a random sample from the whole dataset for

02:17.330 --> 02:17.780
our training.

02:18.890 --> 02:25.550
What part does is it figures out which unique values why has to one, two and three for each of these

02:25.550 --> 02:34.650
values, it's going to go ahead and find the values from Y where that specific row equals this class.

02:34.670 --> 02:36.930
So for each of these classes, we are going to do this.

02:37.000 --> 02:38.820
OK, and what are we going to do?

02:38.930 --> 02:44.480
We are going to do a random subsequence from these values and we are only going to pick 70 percent of

02:44.480 --> 02:46.490
those because we point seven to it.

02:46.640 --> 02:47.720
OK, so.

02:48.900 --> 02:53.910
For number one, it's going to be 70 percent of the values for class number two, it's going to pick

02:53.910 --> 02:57.890
70 percent of the values and four, class number three and all of those row IDs.

02:57.890 --> 03:00.660
So these are just the IDs, not the whole rows.

03:00.960 --> 03:03.630
So those are going to be pushed into our keep IDs.

03:03.790 --> 03:07.380
OK, let's do that and you'll see what this means for each of these classes.

03:07.600 --> 03:09.360
We are going to get these values.

03:09.360 --> 03:11.100
So two, three, four, six, seven.

03:11.100 --> 03:12.840
And all of these some of these are going to be fun.

03:12.840 --> 03:14.910
Class one from classroom class three.

03:15.070 --> 03:19.920
The reason we do it like this is, for instance, if you have an imbalance dataset and like 90 percent

03:19.920 --> 03:26.280
of your data comes from one class and we just do a random split, then it's highly unlikely that most

03:26.280 --> 03:28.380
of the values are going to come from just one class.

03:28.620 --> 03:35.220
So upper class split ensures that we have a good representation of classes in our training and in our

03:35.400 --> 03:36.060
testing sets.

03:36.210 --> 03:38.910
OK, so we just get these IDs back.

03:39.510 --> 03:45.660
We are going to calculate the test IDs now by calculating the differences of all the values from the

03:45.660 --> 03:46.020
training.

03:46.500 --> 03:53.340
So from 100 to 150, these are the training rows and the rest are going to be the districts.

03:53.360 --> 03:54.090
So that makes sense.

03:54.140 --> 03:59.860
OK, so we have two, three, four here, so one, eight, nine goes over here.

04:00.060 --> 04:03.340
OK, so eight, nine is not going to be over there and so on and so forth.

04:03.630 --> 04:06.760
So now we have the IDs for training and testing both eight.

04:06.840 --> 04:12.240
So if you do the values or one hundred and three go here and forty seven go here, which is like a 70

04:12.240 --> 04:12.810
30 split.

04:12.960 --> 04:17.970
OK, then we need another helper function for calculating the actual value.

04:18.260 --> 04:19.590
So before looking at this function.

04:19.620 --> 04:20.630
Let me give you an example.

04:20.640 --> 04:24.780
So let's say you have a machine learning algorithm and you put the value in there and it predicts three

04:24.780 --> 04:26.820
point one because it's real numbers.

04:26.820 --> 04:30.690
It can predict zero point five, it can predict zero point three, it can predict three point one.

04:31.080 --> 04:32.880
It can predict all of those different values.

04:33.180 --> 04:35.300
And we can only have the classes one, two and three.

04:35.430 --> 04:42.120
So we need to write a function which given three point one, will return either one, two or three to

04:42.120 --> 04:42.350
us.

04:42.990 --> 04:44.130
This is that function.

04:44.130 --> 04:47.550
But before we go into this, just look at what argument does so.

04:47.550 --> 04:51.620
If you say amen off this list, it's going to calculate the minimum.

04:51.630 --> 04:54.090
So the minimum of these values is going to be zero point nine.

04:54.090 --> 04:57.060
But we're not interested in the actual value, zero point nine.

04:57.060 --> 05:00.120
We are interested in the position of this one, two and three.

05:00.480 --> 05:01.740
So if you say Aardman.

05:03.140 --> 05:09.370
It's going to return three to us, OK, and amen of, let's say, minus one point one is going to be

05:09.380 --> 05:09.760
two.

05:09.800 --> 05:14.120
So Ogwyn essentially determines the position corresponding to the minimum.

05:14.170 --> 05:16.180
OK, so once we have this, let's go over here.

05:16.490 --> 05:19.010
So we give it a predicted value, let's say three point one.

05:19.250 --> 05:25.420
It's going to calculate the difference from the predicted value and all of these values.

05:25.430 --> 05:28.730
And this is going to be distributed to three point one is going to subtract one.

05:28.960 --> 05:34.640
This is going to be two point one, three point one minus two is going to be one point one and three

05:34.640 --> 05:35.090
point one.

05:35.090 --> 05:36.860
Minus three is going to be zero point nine.

05:37.340 --> 05:42.290
So once we do an absolute of that, because we're only interested in absolute differences and then an

05:42.290 --> 05:47.660
argument is going to return either one, two or three depending on whatever the closest value is.

05:48.500 --> 05:50.810
So we do that and then we say.

05:53.630 --> 05:54.620
Assigned glass.

05:56.590 --> 06:02.260
And if you say one, it's going to say one, if you say one point to, it's going to say one.

06:02.410 --> 06:06.350
If you say one point seven, it's going to now say two because it's the closest.

06:06.940 --> 06:13.090
So similarly, two point seven is going to be three and two point two is going to be two to hold.

06:13.090 --> 06:13.630
This makes sense.

06:13.900 --> 06:18.490
This is because our machine learning algorithms are going to run this value to us and we need to convert

06:18.490 --> 06:19.760
this into an actual class.

06:20.050 --> 06:22.080
So this very simple function does that for us.

06:22.930 --> 06:26.070
Now, let's go ahead and do the actual usage of classifier.

06:26.080 --> 06:27.460
So first we're going to use the decision.

06:28.090 --> 06:29.520
So it's very straightforward.

06:29.530 --> 06:35.590
Now you're going to do decision trees and the model is going to be decision to classify it with a maximum

06:35.660 --> 06:35.980
of two.

06:36.280 --> 06:40.480
If you don't know what a decision tree is, you can look it up and you can study the model.

06:40.480 --> 06:41.650
But it's a very powerful model.

06:41.650 --> 06:43.500
It's very useful for us here.

06:43.510 --> 06:48.070
It's not important what the decision tree algorithm does and how it works for us.

06:48.070 --> 06:49.220
We are just trying to use it here.

06:49.280 --> 06:53.110
OK, so we create the model and we pass it.

06:53.110 --> 06:59.230
The values so fit function is going to do the actual learning and this is going to be the model that

06:59.230 --> 06:59.800
we are using.

07:01.000 --> 07:05.800
We are going to give it all the trained values and the labels for different values.

07:05.800 --> 07:11.110
So you will remember we had trained IDs, so we are going to take all the train it rolls and all the

07:11.110 --> 07:16.570
columns from X and the train is rose from Y, so that's all there is to it.

07:16.900 --> 07:18.820
We do that and we get the result.

07:19.690 --> 07:22.480
So I probably didn't run it here, so.

07:26.320 --> 07:27.500
So I have a typo here.

07:28.310 --> 07:30.390
So there you go.

07:30.880 --> 07:37.540
So this is now done and now we can go ahead and do the testing so we can say CU is equal to the cutie's.

07:37.540 --> 07:43.230
You got to test it and all the rules and then we are going to be here.

07:43.630 --> 07:45.220
So we are going to do the prediction.

07:45.230 --> 07:52.060
So prediction on this model for discovery is this over here, OK, we can find the accuracy and finding

07:52.060 --> 07:54.490
accuracy again is a very simple function.

07:54.520 --> 07:57.160
We give it the predicted values and the ground truth values.

07:58.270 --> 08:03.460
We are going to check how many of the predicted values equals piecewise, equals ground truth, values

08:03.460 --> 08:05.860
and divide by the total number of grouted values.

08:05.860 --> 08:08.860
So that's all that accuracy is the number of correct answers.

08:09.020 --> 08:17.740
OK, so we can say predictions which we calculated over here and these are ground rules.

08:18.010 --> 08:18.880
So once we do that.

08:20.220 --> 08:26.700
I really should fix this, so once we do that, we get the accuracy of ninety three point six, which

08:26.700 --> 08:32.070
is really good decision, trees can be combined into random forests and using that, again, is very

08:32.070 --> 08:32.610
straightforward.

08:32.790 --> 08:37.120
So you get a random forest classifier and the number of trees in this classified is going to be 20.

08:37.590 --> 08:39.490
We are going to fit it just as before.

08:40.230 --> 08:41.250
So here we go.

08:42.710 --> 08:46.850
And we are going to calculate the accuracy for.

08:48.020 --> 08:54.710
This model is what I do, ninety point six support vector machines are also supported by default.

08:54.720 --> 08:56.660
So you can do using libbers SVM.

08:58.170 --> 09:04.860
You calculate the training and testing values, so those go into training and we simply pass them over

09:04.860 --> 09:05.110
here.

09:05.240 --> 09:08.190
OK, the reason we are doing this is Leyb.

09:08.190 --> 09:11.890
SVM requires the structure in a slightly different format.

09:11.910 --> 09:13.020
So we have to transpose this.

09:13.210 --> 09:17.550
So you just have to remember that labor system is slightly different because it's traditionally done

09:17.550 --> 09:17.900
like that.

09:17.950 --> 09:25.230
OK, and we can go ahead and do the learning and then we can do the accuracy test as well to see how

09:25.270 --> 09:27.000
Livestream does on this data set.

09:27.240 --> 09:31.020
OK, so we get ninety five point seven four on live SVM really quickly.

09:31.380 --> 09:35.130
So people don't typically do these traditional machine learning models.

09:35.130 --> 09:38.610
But if you do want to do that, you can see that the code is very clean, very straightforward.

09:38.850 --> 09:41.290
You can work with this in the next video.

09:41.290 --> 09:46.470
We are going to move on to neural networks and that is where machine learning really shines in Julia,

09:46.680 --> 09:52.800
because Julia starts with a very simple concept and that really explodes into the state of the art machine

09:52.800 --> 09:53.010
learning.

09:53.040 --> 09:54.450
So in the next video, we're going to do that.
